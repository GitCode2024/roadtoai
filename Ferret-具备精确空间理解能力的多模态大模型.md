# Ferret：具备精确空间理解能力的多模态大模型
>原文：Ferret: Refer and Ground Anything Anywhere at Any Granularity
>链接：https://arxiv.org/abs/2310.07704
>作者：Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang
>翻译：GitCode

## 摘要
我们推出了 Ferret，这是一个全新的多模态大型语言模型，能够在图像中识别和定位任何形状和大小的空间指代，并能准确地对开放词汇描述进行锚定。为了在大型语言模型框架内统一指代和锚定，Ferret 使用了一种创新而强大的混合区域表示方法，它将离散坐标和连续特征结合起来，共同描绘图像中的区域。为了提取各种区域的连续特征，我们设计了一个空间感知视觉采样器，能够有效处理不同形状间的稀疏度差异。因此，Ferret 能够接受包括点、边界框和自由形状在内的多种区域输入。为了增强 Ferret 的功能，我们精心构建了 GRIT 数据集，这是一个包含 1.1M 样本的全面指代和锚定指令微调数据集，其中蕴含了丰富的层次化空间知识，以及 95K 的硬负数据来增强模型的鲁棒性。这个模型不仅在传统的指代和锚定任务上表现出色，而且在需要区域定位的多模态交流任务上远超现有的多模态大型语言模型。我们的评估还发现，它在描述图像细节方面的能力有了显著提升，并且大幅减轻了对象虚构的问题。相关的代码和数据将可在 https://github.com/apple/ml-ferret 上公开获取。
![图片2.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/0175fd1f-82e0-4d1f-b6e0-ffc31aa0ef01/图片2.png '图片2.png')
<center> 图1：Ferret让多模态大型语言模型（LLM）具备了指引用和定位的功能。在指引用方面，用户可以精确地用点、框或任意自定义形状来指代一个区域或对象。输入中的"regionN"将被转换成一种混合表示形式，然后传递给LLM进行处理。至于定位功能，Ferret能够精确地根据开放词汇的描述来确定对象位置。输出中的"boxN"展示了预测出的边界框的坐标。</center>

## 1 简介
在视觉与语言学习领域，探索如何让模型具备对空间的理解能力，是一个关键的研究课题。这一课题带来了两个重要的能力需求：引用和定位。引用能力指的是模型需要能够精确地理解特定区域的语义信息（参见Krahmer & Van Deemter, 2012; Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016; Zellers et al., 2019的研究），而定位能力则是指模型能够根据语义描述准确地找到对应的区域位置（参见Luo & Shakhnarovich, 2017; Nagaraja et al., 2016; Yu et al., 2017; Kamath et al., 2021的研究）。<br>
本质上，引用和定位需要的是相同类型的知识：将空间信息与语义内容进行对齐。然而，目前的研究大多数情况下是分别学习引用和定位（参见Li et al., 2022; Wu et al., 2022; Yu et al., 2017的研究）。与此相比，人类能够从一个任务中学习，并将学到的共享知识轻松地应用到另一个任务中，能够自然而然地将引用和定位能力融入到日常对话和推理过程中（参见Zellers et al., 2019的研究）。受到这种差异的启发，本文将探讨三个主要问题：（1）如何在同一个框架下整合引用和定位，它们是否能够相互增强？（2）如何有效地表示人类在引用时常用的多种区域类型，例如点、框、涂鸦，甚至是自由形状？（3）如何让引用和定位具有开放词汇、遵循指令和鲁棒性，这些特性对于实际应用来说非常关键？<br>
为了应对这三个关键问题，我们推出了一款名为Ferret的新型多模态大型语言模型（MLLM），它具备引用和定位的功能。我们选择MLLM作为Ferret的核心，因为这类模型具备强大的视觉与语言综合理解能力。为了实现引用和定位的统一，Ferret首先采用自然语言数字形式来表示区域的坐标，正如图3所展示的那样。但是，使用单一点或矩形框坐标来呈现多样化的区域形状，比如笔画、涂鸦或复杂多边形，效率并不高。这些形状对于实现更广泛和精确的人机互动至关重要。为了克服这一难题，我们进一步开发了一个能够感知空间关系的视觉采样器，它能够捕获任意形状区域的视觉特征，并考虑到这些形状内部的不同稀疏度。接着，将离散的坐标与连续的视觉特征相结合，以此在Ferret中构建出一个混合型的区域表示方式。配备了这些方法，Ferret能够处理既包含引用区域又包含自由格式文本的输入，并且在输出时能够自然而然地为每个可定位的对象生成坐标，同时生成文本。据我们所知，Ferret是首个能够处理自由格式区域输入的MLLM模型。<br>
为了提升Ferret在引用和定位方面的能力，使其能够应对开放词汇、遵循指令以及具备鲁棒性，我们精心构建了GRIT数据集，这是一个包含110万个样本的地面和引用指令微调数据集。GRIT数据集涵盖了从简单到复杂的不同空间知识层次，包括对象、关系、区域描述以及需要进行复杂推理的场景。数据集中不仅包含了文本输入、地点输出（定位）的数据，也包含了地点输入、文本输出（引用）的数据，还有在输入和输出中同时混合地点和文本的数据。大部分数据集是从现有的视觉或视觉-语言任务转换而来，如物体检测和短语定位，并且通过精心设计的模板使其能够遵循指令。此外，我们还利用ChatGPT/GPT-4（由OpenAI开发）收集了34K个引用和定位指令微调对话，以帮助训练出一个能够遵循指令且具备开放词汇的引用和定位通才。为了进一步提高模型的鲁棒性，我们还进行了空间感知的负数据挖掘。<br>
Ferret具备了强大的空间理解和定位能力，能够在开放词汇的条件下进行操作。在标准的引用和定位任务中，它的表现远超其他模型。更重要的是，我们认为这种引用和定位的能力应该被整合到人类的日常交流中，比如当人们提到他们不熟悉的事物并询问其用途时（如图1）。为了测试这种新能力，我们开发了Ferret-Bench，它包含了三种新的任务类型：引用描述、引用推理以及对话中的定位。我们对现有的多语言学习模型（MLLM）进行了基准测试，发现Ferret的平均表现比最佳模型高出20.4%。此外，Ferret还显示出一种有趣的特点，即它能够减少对象幻觉的问题。<br>
总的来说，我们的研究贡献可以归纳为三个方面。首先，我们开发了Ferret，这是一种采用了混合区域表示并搭配了一种创新的空间感知视觉采样器的模型，旨在MLLM中实现精细化和开放词汇的引用和定位功能。其次，我们创建了GRIT，这是一个庞大的地面和引用指令微调数据集，用于训练模型，并且包含了额外的空间负样本，以提高模型的稳健性。最后，我们推出了Ferret-Bench，这是一个用于评估那些需要同时进行引用/定位、语义理解、知识掌握和推理能力的任务的平台。我们的模型在这些任务中展现出了卓越的性能，并且有效减少了对象幻觉的问题。<br>
表1：Ferret与近期集成空间感知的MLLMs的比较。‘常规’指的是使用模板转换的公开可用数据的综合集合，‘GPT-生成’表示使用GPT生成的引用/定位数据集，而‘鲁棒性’指的是旨在缓解幻觉并提高鲁棒性的数据集。第4节详细解释了每个数据集的更多细节。<br>
![截屏2023-12-27 18.13.00.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/39361a2d-b8d5-4f35-8c7d-36120f18c2e4/截屏2023-12-27_18.13.00.png '截屏2023-12-27 18.13.00.png')

## 2 相关工作
**多模态大型语言模型（MLLMs）**。大型语言模型（LLMs），包括 GPTs (Brown et al., 2020; OpenAI, 2023a), PaLM (Chowdhery et al., 2022), BLOOM (Scao et al., 2022), and LLaMA (Touvron et al., 2023a; b)，已经彻底改变了自然语言处理（NLP）的研究领域，并推动了多模态语言模型的发展。早期的模型主要集中在大规模的图像和文本预训练。一些著名的模型包括 imVLM (Wang et al., 2022c), GIT (Wang et al., 2022a), PaLI (Chen et al., 2022b), PaLI-X (Chen et al., 2023c), BLIP-2 (Li et al., 2023c), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023), CM3 (Aghajanyan et al., 2022), and CM3Leon (Yu et al., 2023) 。其中，Flamingo 通过引入门控交叉注意力机制，将预训练的 CLIP 图像编码器与 LLMs 结合，展示了强大的多模态上下文少样本学习能力。它的开源版本，如 OpenFlamingo (Awadalla et al., 2023) 和 IDEFICS (Laurençon et al., 2023)，也受到了极大的关注。这些模型通常使用数百万甚至数十亿个图像-文本对以及交错图像-文本数据集进行预训练。<br>
与此同时，最新的研究趋势是利用预训练的 LLMs 进行视觉任务的指令微调。一些显著的模型包括 LLaVA (Liu et al., 2023b), MiniGPT-4 (Zhu et al., 2023a), mPLUG-Owl (Ye et al., 2023), Otter (Li et al., 2023a), InstructBLIP (Dai et al., 2023)等。此外，新的模型如 FROMAGe (Koh et al., 2023b), GILL (Koh et al., 2023a), Emu (Sun et al., 2023) 已经扩展了 MLLMs 的功能，使其能够进行图像检索和生成。有关这些模型的详细讨论，请参阅 Li et al. (2023b) 的研究报告中的第5章。<br>
**用于引用和定位的MLLMs**。在当前的学术领域，像 Kosmos-2 (Peng et al., 2023) 和 Shikra (Chen et al., 2023b) 这样的研究与我们的工作非常接近，因为它们同样使得多模态大型语言模型（MLLMs）能够进行精细的图像理解以及开放世界的引用和定位任务。此外，还有 GPT4ROI (Zhang et al., 2023), PVIT (Chen et al., 2023a), BuboGPT (Zhao et al., 2023), VisionLLM (Wang et al., 2023), 和 ContextDET (Zang et al., 2023) 等研究也在此方向上做出了贡献。但是，我们的模型有几个显著的不同点。首先，以往的研究只支持边界框（以及 Shikra 中的点）作为输入。而我们的 Ferret 模型，由于其创新的混合区域表示方法，能够支持更多种类的自由形状进行引用，包括点、框、草图、涂鸦、多边形等。其次，我们精心构建了一个广泛的引用和定位指令微调数据集。第三，我们推出了 Ferret-Bench，以促进未来在这一领域的研究，并提高评价标准。最后，与之前的研究相比，我们的模型在性能上更胜一筹，尤其是在显著减少对象虚构方面。一个更直接的对比分析可以在 Tab. 1 中找到。<br>
**统一定位与视觉语言理解**。我们的研究还与之前致力于将文本和边界框输出整合到视觉语言（vision-language，VL）模型中的工作有关，例如 UniTAB (Yang et al., 2022), OFA (Wang et al., 2022b), 和 Unified-IO (Lu et al., 2022)。这些模型同样采用了额外的离散标记来表示边界框，类似于 Pix2Seq (Chen et al., 2021; 2022a) 的方法。Ferret 的独特之处在于：（1）我们的模型建立在 LLMs 之上，将 LLMs 的强大功能和定位相结合，从而开辟了新的可能性，例如基于定位的指令微调；（2）我们将边界框坐标视为常规文本标记，这样就不需要额外的特殊标记来专门表示边界框。

## 3 方法
我们先从阐述我们创新的混合区域表示方法入手，该方法能够精细地描绘出多种形状和大小的区域。随后，我们进一步介绍了 Ferret 的模型结构。

### 3.1 混合区域表示
![x2.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/a501cf6f-262a-4d7a-b646-f67fb869ab51/x2.png 'x2.png')
<center>图2展示了边界框与自由形态的对比。这两个物体虽然拥有相似的边界框，但在依赖边界框进行识别时会引发混淆。得益于混合区域表示技术的应用，Ferret 能够有效地将它们区分开来。</center>

在引用特定区域时，常用的三种格式是点、边界框和自由形状。点和边界框格式可以通过坐标精简地表达（例如，点的坐标为[x, y]，边界框的坐标为[x_min, y_min, x_max, y_max]），这一点在Peng et al. (2023) 和 Chen et al. (2023b) 的研究中有所体现；而自由形状则具有更高的适应性，它包括了涂鸦、多边形和遮罩等多种区域类型。自由形状的优势在图2中一目了然。使用坐标来描述自由形状在计算上既耗时又难以精确，其复杂性反而阻碍了模型学习如何将提供的坐标与相应区域建立起清晰的关联。<br>
为了涵盖这三种不同的格式，我们提出了一种混合区域表示方法，它将离散的坐标与连续的视觉特征相结合，以便指代特定区域，具体展示在图3的左上角。在处理坐标时，我们遵循 Chen et al. (2021) 和 Yang et al. (2022) 的研究，将每个坐标细分为 n<sub>bins</sub> 个离散的区间。对于连续视觉特征，我们首先为给定区域 𝐑 构建一个与图像尺寸相同的2D二值掩码 𝐌 ，目标区域内标记为1，区域外标记为0。接着，将这个二值掩码 𝐌 与提取的图像特征图 𝐙 一同输入到我们提出的空间感知视觉采样器 s ( ⋅ ) 中，该采样器将在第3.2节中详细介绍，以此提取视觉连续特征 𝐟 = s(𝐌,𝐙)。<br>
 最终，我们用一个点 { x,y, 𝐟<sub>R<sub>p</sub></sub>} 来表示，这里的区域 R<sub>p</sub> 是一个以 {x,y } 为圆心、半径固定的圆形区域。边界框或自由形状都可以用 { x<sub>min</sub>,y<sub>min</sub>,x<sub>max</sub>,y<sub>max</sub>,𝐟<sub>R<sub>box</sub></sub>} 来表示，其中  x<sub>min</sub>/x<sub>max</sub> 指的是区域在 x 轴的最小和最大坐标，对于 y 轴亦是如此。R<sub>box</sub> 指代输入的区域。
 ![x3.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/839219fe-c6cc-4c63-b096-72f06c298a3b/x3.png 'x3.png')
 图3：Ferret模型架构概述。（左侧）我们提出的结合了离散坐标和连续视觉特征的混合区域表示，以及能够感知空间信息的视觉采样器。（右侧）整个模型的架构。在模型中，除了图像编码器的参数外，其他所有参数都可以通过训练进行优化。
 
### 3.2 模型架构
如图3所示，Ferret模型的核心构成包括：（一）图像编码器，负责从图像中提取关键特征；（二）我们提出的一种具有空间感知能力的视觉采样器，它能够识别并提取出图像中的区域连续特征；以及（三）一个大型语言模型（LLM），它能够同时处理并建模图像、文本以及区域特征之间的复杂关系。<br>
**输入**。在输入阶段，我们将图像输入到经过预训练的视觉编码器 CLIP-ViT-L/14  (Radford et al., 2021) 中，以此提取图像的特征嵌入  𝐙 ∈ℝ<sup>H×W×C</sup>。对于文本数据，我们利用预训练LLM的分词功能对文本序列进行分割，并将分割后的结果映射成文本嵌入 𝐓 ∈ ℝ<sup>L×D</sup>。至于提到的特定区域，我们在区域名称之后添加了坐标信息和一个特殊的标记（SPE），以预留空间用于后续的连续特征提取。例如，我们可能会标记为“a cat [100, 50, 200, 300] ⟨ SPE ⟩ ”。如果区域名称不明确或难以准确描述，例如区域内包含多个物体，我们可以简单地将“区域”或“区域”作为“ ⟨ region_name ⟩ ”。通过这种处理方式，所指的区域可以无缝地融入普通文本之中，构成完整的语句。<br>
**空间感知视觉采样器**。我们提出了一种空间感知视觉采样器，以应对所指区域可能具有的多种不规则形状，这些形状不仅限于简单的点或矩形框。传统的基于网格的处理技术，如卷积或patch注意力机制，并不适用于处理这种不规则的形状。与我们面临的情形相似，3D点云也通常呈现出不规则的形态，并且在三维空间中展现出不同的稀疏度。借鉴了3D点云学习领域的相关研究（Qi et al., 2017a; Ma et al., 2022; Wang et al., 2019），我们设计了一种能够感知空间分布的视觉采样器。<br>
在提取的图像特征图 𝐙 ∈ ℝ<sup>H×W×C</sup> 和二值区域掩码 𝐌 的基础上，我们首先在 𝐌 中随机选取 N 个正点。每个点的特征通过双线性插值方法获得。这 N 个点随后被送入一个由多个模块组成的级联网络，每个模块包含三个主要步骤：采样、聚集和池化。（1）采样：我们采用最远点采样（FPS）算法（Qi et al., 2017b）从 N 个点中抽取 N/r 个点，以确保采样点的覆盖面足够广泛。 （2）聚集：对于每个被抽样的点 x<sub>i</sub>，我们在之前采样的 N 个点中寻找其 k 个最近的邻居点，从而形成一组点 {x<sub>i1</sub>, x<sub>i2</sub> , … , x<sub>ik</sub>} 。接下来，受到 PointMLP（Ma et al., 2022）的启发，我们针对每一组点融合采样点 x<sub>i</sub>, 和其邻居点的特征，具体方法如下：
![截屏2023-12-27 19.08.48.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/8ecc0c6b-d880-4e22-a545-e699876dde05/截屏2023-12-27_19.08.48.png '截屏2023-12-27 19.08.48.png')

在此过程中，x<sub>ik</sub> 代表 x<sub>i</sub> 的一个邻近点， 𝐙 ( x ) 指的是点 x<sup>’</sup>s  的特征（在第一个模块中，这是通过从特征图 𝐙 中插值得到的；在后续模块中，这是前一个模块的输出特征）， C (x) 表示点 x 的二维坐标， [ ; ] 表示多个向量的通道级连接， $\theta$ 通过一个线性层来调整相对局部的特征， $\sigma$ 也是一个线性层，用于将每个邻居点的局部特征与采样点的特征相结合。 （3）池化：执行最大池化操作，将 k 个邻居点的特征合并为一个特征，以此作为采样点的表征。
![截屏2023-12-27 19.19.13.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/4b7f7a7f-a1ab-4528-b970-36114debb294/截屏2023-12-27_19.19.13.png '截屏2023-12-27 19.19.13.png')

经过这三个步骤，我们虽然减少了点的数量，但每个点的特征表示却变得更加丰富，因为这一过程不仅融合了局部邻居点的特征，还考虑了它们之间的相对位置。在实验配置中，我们将参数设定为 N= 512，r=4， k=24，并串联两个这样的模块，最终生成了32个带有各自特征的点。与 ROIAlign (He et al., 2017) 的方法类似，我们将这些点的特征压缩成一个单一的向量，并将其映射到LLM嵌入的维度。这个最终的特征向量取代了输入数据中的 ⟨ SPE ⟩ 标记。<br>
**输出**。述区域表示在Ferret输入中用于引用特定区域。在Ferret的输出中，为了确保信息与图像中的具体位置相对应，我们在文本描述中提及的特定区域或名词后面，立即提供了边界框的坐标。例如，“这幅图中有一只狗，它的位置是[100, 150, 300, 200]。”采用这种格式，我们的模型应当能够学习到在当前图像中哪些内容是可锚定的，以及它们各自的位置在哪里。<br>
**LLM**。我们采用 Vicuna (Chiang et al., 2023) 作为我们的语言模型，这是一种仅包含解码器的 LLM (Brown et al., 2020) ，它是基于 LLaMA (Touvron et al., 2023a) 进行指令优化的。在将图像嵌入数据输入到语言模型之前，我们通过一个额外的线性层对图像嵌入进行维度转换，以确保它们与文本标记的嵌入维度相匹配。

## 4 Grit：这是一个结合了定位（grounding）和引用（referencing）的指令调优数据集
在本节中，我们介绍了GRIT，一个包含大约110万多媒体对话的地面和引用指令调优数据集，用于模型训练。GRIT由三种类型的数据组成：（1）公共数据集，它们被转换为指令跟随格式（第4.1节）；（2）通过ChatGPT和GPT-4生成的指令调优数据（第4.2节）；以及（3）用于增强模型鲁棒性的空间负采样的附加数据（第4.3节）。
![x4.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/b055dec9-f9e6-4108-9cf9-655d94939f63/x4.png 'x4.png')
图4：用于Ferret模型训练的GRIT数据集概述。它包含三种类型的数据：（1）公共数据集，这些数据集被转换为指令跟随格式（前3行）；（2）通过提示ChatGPT和GPT-4生成的数据（第4行）；以及（3）用于增强模型鲁棒性的负数据（最后一行）。

### 4.1 层级
 空间理解可以根据细致程度和任务格式的不同层次来划分。在构建我们的数据集时，我们从两个维度出发，考虑了以下几种类型：
- 从细致程度来看，我们将数据划分为四个主要类别：（1）单独的物体，（2）物体间的关系，（3）特定区域的描述，以及（4）基于区域的复杂推理。
- 从任务格式来看，我们将数据进一步细分为三种类型：（1）区域在文本之前的输出数据，（2）文本在区域之前的输出数据，以及（3）文本和区域结合的数据。

我们收集了一个包含大量公共数据的集合，这些数据集中在之前提到的各个维度上，并通过精心设计的模板将它们转换成了能够遵循指令的格式。对于这些模板的更详细说明，请参阅附录A.1。<br>
表2：用于在上下文中学习构建GPT辅助的引用和定位指令调优的一个示例。
![图片3.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/99cbc8bd-7995-4347-91da-e75aeadbee71/图片3.png '图片3.png')

**单个对象**。 为了在对象层面培养视觉理解能力，我们挑选了包括 Visual Genome (Krishna et al., 2017), Object365 (Shao et al., 2019) 在内的对象检测数据集，以及 RefCOCOs (Yu et al., 2016; Lin et al., 2014; Nagaraja et al., 2016) 和 Flickr30k-Entities (Plummer et al., 2015) 等视觉定位数据集。我们对Visual Genome的对象数据进行了转换，使其符合区域-文本输出的格式。此外，为了训练Ferret以理解自由形态的物体，我们采用了 SAM (Kirillov et al., 2023) 技术对Visual Genome的对象数据进行处理，生成了每个物体的分割掩码。这些掩码随后被送入空间感知视觉采样器，以便在训练过程中提取连续的区域特征。而视觉定位数据集和Object365的数据则遵循文本-区域输出的格式。这一部分总共包含了67.8万条数据。<br>
**对象之间的关系和区域的描述**。为了处理这两个方面，我们从 Visual Genome (Krishna et al., 2017)  中挑选了与物体间关系和区域描述相关的数据。这两个数据集都采用了区域-文本输出的格式，我们共收集了17.7万条数据。与Visual Genome的对象数据处理方式相同，我们也使用了SAM从Visual Genome的关系数据中提取了物体的分割掩码。<br>
**针对特定区域的复杂推理**。在处理特定区域的复杂推理时，我们在ChatGPT/GPT-4的协助下创建了一个创新的数据集。该数据集采用了文本和区域相结合的格式，具体细节将在后续部分进行阐述。

### 4.2 利用GPT生成的视觉指令数据
除了通过模板转换现有数据集外，对话指令调优数据对于 MLLM 理解人类意图并生成流畅、自然、长篇大论的回应至关重要（Liu et al., 2023b; Zhu et al., 2023a; Li et al., 2023d）。零样本提示被广泛用于获取视觉指令调优数据，这种方法通过提供图像的文本场景描述和人工标注对话作为零样本演示，然后提示 ChatGPT/GPT4 根据新图像的文本场景描述生成新的对话。<br>
然而，之前的指令调优数据主要关注描述整个图像，而未明确指出与空间相关的具体信息。为了收集指代和地面指令调优数据，我们采取了以下三个步骤来强化基于区域的空间知识。（i）除了通常使用的对象和全局标题之外，我们的符号场景描述还包含了对象之间的物理关系以及它们各自区域的标题和坐标。（ii）在人工标注的对话中，我们在可指代的区域或对象之后添加了坐标，这些坐标可能出现在输入或输出中，或者两者都出现，而且这些对话通常集中在特定区域。这样做有助于引导 ChatGPT/GPT4 在生成新对话时采用相似的策略。我们在数据集中使用的零样本提示示例展示在表2中。（iii）有时生成的对话并不会完全遵循我们设定的系统提示和零样本示例中的规则和模式，这可能是因为 LLM 的输入上下文过长，难以处理所有细节。为了解决这个问题，我们建议再次利用 ChatGPT/GPT-4 来细化初步生成的对话，其上下文长度平均仅为第一轮生成数据的10%。为了节约成本，我们在第一轮生成时使用 ChatGPT，而在细化时则使用 GPT-4。最终，我们收集了总计34k个对话。<br>
另外，为了充分利用现有的指令调优数据，比如 LLaVA（Liu et al., 2023b）中的数据，我们在 LLaVA-158k 数据集上使用了开放词汇对象检测器 GLIPv2（Zhang et al., 2022）来定位文本中的可地面化名词。接着，我们在这些名词后面添加了边界框，从而创建了一个伪地面化的 LLaVA 指令数据集，这个数据集也被用于训练 Ferret。

### 4.3 空间负例挖掘
正如之前的研究（Li et al., 2023e; Liu et al., 2023a）所指出的，MLLM 在回答 yes/no 问题时倾向于产生虚构的内容。我们在询问具体区域时也发现了类似的情况。为了解决这个问题，我们采用了两种方法进行负样本挖掘：（i）基于图像的条件类别定位，和（ii）基于语义的条件类别定位。这两种方法都要求模型定位特定的对象类别，这样模型就能更好地识别和可能识别某些物体的缺失。它们在选择负类别的方法上有所不同。对于（i），我们使用了 Object365 数据，并从词汇表中随机选择在给定图像中未出现的对象类别。对于（ii），我们使用了 Flickr30k 数据，并通过使用 ChatGPT/GPT4 找到与原始类别、属性或数量最相似的实体来获取负类别，例如，“男人” vs. “女人”，“蓝色” vs. “黄色”，“两个” vs. “三个”。<br>
我们整理数据，以确保两种类型的正负样本平衡。总共收集了 95k个数据。更详细的说明在附录 A.2 中提供。

## 5 实验
首先，我们详细阐述了 Ferret 模型的训练细节。在评估阶段，我们从传统引用和定位化基准测试（第 5.1 节和第 5.2 节）开始评估 Ferret 的性能。然后，在第 5.3 节中，我们展示了 Ferret 在更复杂的多模态聊天场景中的引用和定位化能力。每个部分的详细可视化可以在附录 C 中找到。此外，我们在第 5.4 节中消融了 Ferret 的关键组件，分析了 Ferret 的对象虚构问题（第 5.5 节），并讨论了 Ferret 与 GPT-4V 的对比（第 5.6 节）。<br>
**训练细节**。我们使用 CLIP-ViT-L/14@336p 初始化图像编码器，使用 Vicuna 初始化 LLM，使用 LLaVA 的第一阶段权重初始化投影层，而视觉采样器则是随机初始化的。初始化之后，我们在前述的 GRIT 数据集上训练 Ferret 三个周期，采用 Loshchilov & Hutter（2017）的优化方法，学习率为 2e-5，批处理大小为 128。对于 Ferret-13B/7B，训练耗时约为 5/2.5 天，使用 8 个 A100 GPU。在训练过程中，当输入引用区域时，我们随机选择区域的中心点或边界框（如果可用，也可以选择分割掩码）来表示区域。我们在训练数据中执行去重操作，以去除下游评估中出现的样本。

![图片4.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/4f1debe0-9ea5-475d-97b1-ccdc045c45e4/图片4.png '图片4.png')
表 3：三种不同引用类型（包括点、框和自由形状）的引用对象分类结果。'✕’表示没有这种能力。<br>
![图片5.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/0b1b3094-1ea2-46ce-937b-39d7517af885/图片5.png '图片5.png')
表 4：在 Flickr30k Entities 测试集上的地面图像字幕生成结果。BLEU@4、METEOR、CIDEr 和 SPICE 用于字幕评估。F1<sub>all</sub> 和 F1<sub>loc</sub> 用于地面评估。‘–’ 表示未报告。

### 5.1 输入引用
模型在理解指代方面的能力，体现在对于问题中提到的特定区域，它能够多准确地把握该区域的语义。为了评估这一点，我们从最基本的语义单元——对象开始，因为它是基础且容易定义的。具体来说，我们评估的任务是参照对象分类：问题指的是图像中的某个特定区域，模型需要对这个区域中的对象进行分类。由于Ferret和MLLMs通常生成自由形式的文本回应，如果直接让模型在不加限制的情况下进行分类，将预测类别与真实类别匹配是不准确的。因此，我们将其设计为一个二选一的问题，格式为“图像中的对象 ⟨位置⟩ 是一个 ⟨类别A⟩ 还是一个 ⟨类别B⟩？”我们将二选一的问题和图像输入到MLLMs中以获得回应，然后通过某些规则检测回应是否与真实类别（GT）匹配。<br>
为了准备数据，我们使用了 LVIS 数据集 (Gupta et al., 2019)的验证分割，覆盖了超过1000个对象类别，并采样了2667个对象作为真实对象。然后，我们在同一图像中随机选择一个与真实对象中心点接近的不同对象类别作为负对象，并将 ⟨类别A⟩ 和 ⟨类别B⟩ 替换为这两个随机类别以形成2667个问题。此外，为了模仿人类生活中指代的多样性，我们将 ⟨位置⟩ 替换为三种不同类型：点、框和自由形状。对于点，我们在真实对象的内部随机采样一个点，该点也靠近真实对象的边界。对于框，我们使用LVIS提供的真实边界框。对于自由形状，我们在真实对象内部随机生成一些笔画以模拟。所有三种类型的指代结果总结在表3中。Ferret可以显著超越先前的模型（Peng et al., 2023; Chen et al., 2023b）并处理所有类型的指代，这是先前作品中明显缺乏的能力。我们在图5中可视化了一些示例。

### 5.2 输出定位
Ferret在涉及指代的对话中表现出色，这使得它能够被广泛地应用于各种视觉语言（VL）任务，尤其是那些需要确定具体输出位置的任务。为了严格检验 Ferret 的地面定位能力，我们首先将其应用于生成范式下的标准视觉地面任务。接着，为了评估单词与图像区域之间的对应关系，我们进一步在带地面的图像标题任务上对Ferret进行了评估。<br>
视觉地面任务。视觉地面任务旨在将语言查询与图像中的相应区域相对应。我们在三个知名的子任务上进行了实验：指代表达式理解（REC），包括 RefCOCO（Lin et al., 2014）、RefCOCO+（Yu et al., 2016）和 RefCOCOg（Mao et al., 2016），以及短语地面任务，使用 Flickr30k Entities 数据集（Plummer et al., 2015）。在REC任务中，模型需要针对图像中特定区域的描述或问题，预测一个唯一的边界框。而在短语地面任务中，模型则需要将输入句子中的所有名词短语与相应的图像区域框相对应，并预测这些框以及单词与框之间的连接。对于这两个任务，我们使用了统一的提示，即“`<query>/ < phrases>`的位置在哪里？”的形式，其中`< query>` 指代文本中的指代表达式，‘ < phrases> 表示给定短语的“逗号分隔”的集合。模型被训练以“ < query>[ box ] 。”的格式输出。如果生成的边界框与真实框（GT框）的交并比（IoU）超过0.5，则该边界框被认为是正确的。正如表5所示，Ferret在所有评估指标上均取得了卓越的性能，与专门的微调方法（Kamath et al., 2021）不相上下。一些成果在图5中进行了可视化展示。
  
![截屏2023-12-28 11.04.48.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/d70df4a1-5a93-4c5e-bf93-30ff7316f94d/截屏2023-12-28_11.04.48.png '截屏2023-12-28 11.04.48.png')
表5：在理解指代表达（RefCOCO、RefCOCO+、RefCOCOg）和短语指代（Flickr30k实体）任务上，准确率达到了50%的性能对比。*标记表示在第二个阶段，该方法经过了特别的精调优化。

![图片6.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/74ea3efe-26b2-4de9-a481-e080dbeb5092/图片6.png '图片6.png')
图5：展示了Ferret在指代和地面方面的几项能力。更多详细的可视化效果，请参见附录C。<br>
**定位注释任务**。在这个任务中，模型需要生成一段文字描述，并将描述中出现的所有名词短语与图像中的具体区域相对应。最终的预测结果通常包括三个部分：文本描述、视觉区域（以边界框的形式呈现），以及文字与边界框之间的对应关系。我们在Flickr30k Entities数据集上遵循已有的评估标准，分别使用注释评价指标和定位F1得分来衡量模型的注释和定位能力。 F1<sub>all</sub>将定位能力评估为多标签分类问题。此外，我们还报告了  F1<sub>loc</sub>，这一指标只针对正确预测的对象词汇计算定位得分。详细结果汇总在表5.1中，显示Ferret在该任务上取得了领先性能。

### 5.3 FERRET-BENCH：进行多模态交流的同时实现引用和定位

多模态聊天已经成为MLLMs的一项新兴能力。先前的基准测试（Liu et al.,2023b）主要使用GPT-4作为评判员来评估对话、详细描述和复杂推理。然而，目前尚无数据集能够评估需要参照或定位动作的多模态聊天，例如，当个人引用一个不熟悉的对象并询问其用途的情况。为了基准这一有趣且实用的能力，我们引入了FerretBench，它涵盖了三种基于区域的问题，评估参照和定位能力：(i) 参照描述：模型被要求根据其与周围物体的交互来描述一个参照区域。(ii) 参照推理：模型需要在正确的一个或多个参照区域上进行推理。(iii) 对话中的定位地：模型需要正确、准确地推理并接地/定位推理所需的物体/区域。为了便于基准其他方法，我们用盒子来表示区域，而不是点或自由形状。

![截屏2023-12-28 11.43.24.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/0dfcdbd7-260c-4fcc-965e-dfac14982fc2/截屏2023-12-28_11.43.24.png '截屏2023-12-28 11.43.24.png')
表 6：Ferret-Bench上的参照推理可视化结果，展示了不同模型（LLaVA、Kosmos-2、Shikra和Ferret（我们自己的））之间的区别。为了清晰地表达，我们在模型输出中省略了生成的边界框。更多可视化内容见附录C。<br>
具体而言，我们从COCO验证集中为每种问题类型随机选择了40张图片，并依照第4.2节中的指令生成流程生成了问题和GPT-4的答案。按照 Liu et al.（2023b）的研究方法，我们将问题和图片输入到MLLMs中，以获取预测答案，接着让GPT-4根据真实的文本场景描述（包括对象、关系、区域描述和全局描述）对预测答案和GPT-4生成的伪答案进行评分。GPT-4评估了参照理解的准确性、对象定位的精确性和语义的正确性。评分从1到10分，分数越高表示表现越好。我们计算预测答案的得分与GPT-4答案得分的比例，并以百分比形式展示，以此来衡量MLLMs的性能。我们还让GPT-4对评分进行了全面审查，发现GPT-4在评估空间精确度方面表现优异，例如预测边界框与真实边界框坐标的偏差。更多详细说明请参见附录B。<br>
我们利用LLaVA-Bench（Liu et al.,2023b）和我们新提出的Ferret-Bench来对比Ferret与先前模型的表现，这些模型包括LLaVA（Liu et al.,2023b）、Shikra（Chen et al.,2023b）和Kosmos2（Peng et al.,2023）。结果汇总在表7中。Ferret在所有任务类型中均展现出卓越的性能，将详细描述类别的得分从68.3分提高到了80.9分，特别是在那三项新的要求参照和接地能力的任务上表现得尤为突出。表6展示了一个可视化对比示例，其中Ferret展现了其在空间理解和常识推理方面的强大能力。<br>

![截屏2023-12-28 11.56.39.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/193b9cad-9933-49ed-97f8-f3c85403e596/截屏2023-12-28_11.56.39.png '截屏2023-12-28 11.56.39.png')
表7：通过GPT4作为评判员在LLaVA-Bench和提出的Ferret-Bench上的评估结果<br>
![截屏2023-12-28 11.57.37.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/8d2f7888-ce7a-4b84-b0b9-3cda1dec6ed4/截屏2023-12-28_11.57.37.png '截屏2023-12-28 11.57.37.png')
表8：定位数据和引用数据相互增益的中断研究。
![截屏2023-12-28 11.59.22.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/9c9e7cb8-7e7c-4113-be8b-37a37f7b5259/截屏2023-12-28_11.59.22.png '截屏2023-12-28 11.59.22.png')
表9：所提出空间感知视觉采样器有效性的消融研究。

### 5.4 消融
在以下的消融研究中，我们默认消融了Ferret-7B模型，并在Flickr30k Entities验证集上主要针对引用对象分类和定位任务进行了评估。<br>
定位和引用能力之间的相互促进。如表8所示，本文强调的定位和引用两个主要能力实际上能够相互增强。特别是，在训练过程中引入定位数据后，引用性能有所提升，反之亦然。空间感知视觉采样器。我们通过用SEEM（Zou et al., 2023）中的视觉采样器替换空间感知视觉采样器，来评估其有效性。SEEM方法将所有采样点的特征平均值作为区域特征。如表9所示，我们的方法在所有三个引用任务上的表现均优于先前的视觉采样器。<br>
LLM模型大小。我们研究了LLM模型大小对定位和引用任务性能的影响。如表3-7所示，拥有更大的LM骨干网络通常有助于提高性能。

### 5.5 物体幻觉
得益于对细粒度空间知识和负样本挖掘的整合，Ferret 在应对幻觉问题上表现出了显著的效果。我们在 POPE 基准测试（Li et al., 2023e）上对物体幻觉进行了评估。具体结果汇总在表10中。Ferret 的表现堪比 Shikra（Chen et al., 2023b），并且明显优于近期流行的 MLLMs。

### 5.6 在引用和定位任务方面，Ferret 与 GPT-4V(ision) 的快速对比分析
最近，GPT-4 推出了其多模态版本，公众称之为 GPT-4V。在一篇后续的技术论文（Yang et al., 2023）中，简要讨论了 GPT-4V 的基础定位能力。在本节中，我们将通过若干示例来检验 GPT-4V 的引用和定位功能，并将其与 Ferret 进行对比。在引用方面，GPT-4V 通过两种方式接受提示：（i）图像中用红色圆圈或轮廓标出的区域，问题会询问有关这个红色标记区域的信息。（ii）图像保持静态，但问题中会提供图像的尺寸和坐标，以便引用特定的区域。至于定位方面，我们遵循 Yang et al. (2023) 的指示，即“使用边界框在图像中定位 ⟨类⟩。图像尺寸为（宽度，高度）”。<br>

![截屏2023-12-28 14.13.42.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/4e7f62ae-7fd9-43ce-ae8a-5d96a9009a3e/截屏2023-12-28_14.13.42.png '截屏2023-12-28 14.13.42.png')
表10：展示了在使用 POPE 评估流程（Li et al., 2023e）对物体幻觉进行基准测试的结果。<br>

![截屏2023-12-28 14.13.48.png](https://raw.gitcode.com/lovinpanda/TheRoadtoAI/attachment/uploads/2c93b5f3-8e35-4c7e-984e-697a37070899/截屏2023-12-28_14.13.48.png '截屏2023-12-28 14.13.48.png')
图6：展示了 Ferret 与 GPT-4V 在引用和定位任务上的对比。在使用 GPT-4V 时，我们通过在图像上用红色边框圈出区域或在文本中提供它们的坐标来引用特定区域。在引用能力上，我们发现 GPT-4V 在识别和理解较小区域上存在局限性。在定位任务上，GPT-4V 也无法在复杂环境中准确定位较小的物体。<br>
我们的观察显示，GPT-4V 能够在一定程度上通过图像中的彩色区域或文本中的坐标来理解引用。然而，相比于 Ferret，GPT-4V 在对小区域进行精确引用理解时显得不足，例如在摩托车中的“减震器”（见图6顶部示例）。另一方面，GPT-4V 在常识方面表现出更丰富的知识，例如，它能进一步指出排气管可以减少噪音，这种细微的差别可能是由于 GPT-4 的语言能力得到了增强。在定位任务方面，我们用 CAPTCHAs 对 GPT-4V 进行了测试，这也是 Yang et al. (2023) 中提到的一项挑战。在交通信号灯的例子中，Ferret 能够在复杂的场景中准确识别大多数交通信号灯，如图6底部示例所示。<br>
尽管如此，在通用问题回答的领域，GPT-4V 的表现格外引人注目。它不仅能够熟练地处理首个问题，还能够针对与特定区域相关的后续问题提供深入且详尽的回答。但是，在需要精确的边界框来进行地面定位时，Ferret 特别凸显其优势，它能够满足那些对较小区域内的精度要求极高的应用场景。这正是 Ferret 发挥作用，弥补不足之处。

## 6 总结
我们推出了 Ferret，一个新型的多模态大型语言模型，它在引用和定位任务方面表现出色。Ferret 能够引用图像中任何形状的区域，并且能够自动为模型认为可以进行定位的文本建立地面关系。我们为模型的训练精心准备了 GRIT 数据集，并为评估创建了 Ferret-Bench 数据集。与大多数多模态语言模型一样，Ferret 可能会生成有害或与事实相反的回应。展望未来，受到 LISA（Lai et al., 2023）的启发，我们打算进一步提升 Ferret 的能力，使其在提供边界框的基础上，还能生成分割掩码。














